{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K28fzbB_NXOr"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "import pandas as pd\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed data saved to preprocessed_normal_data.csv\n",
            "Preprocessed data saved to preprocessed_attack_data.csv\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "def preprocess_and_save(input_file, output_file, encoder, scaler, categorical_columns):\n",
        "    \"\"\"\n",
        "    Preprocess the input CSV file by applying OneHotEncoding to categorical columns,\n",
        "    MinMax scaling to non-categorical columns, and combining them into a final dataset.\n",
        "    \"\"\"\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(input_file, encoding='utf-8', nrows=10000)\n",
        "\n",
        "    # Save the first and last columns separately\n",
        "    first_column = df.iloc[:, 0]\n",
        "    last_column = df.iloc[:, -1]\n",
        "\n",
        "    # Exclude the first and last columns for preprocessing\n",
        "    data = df.iloc[:, 1:-1].copy()\n",
        "\n",
        "    # Process categorical columns with OneHotEncoder\n",
        "    encoded_features = encoder.transform(data[categorical_columns])\n",
        "    encoded_feature_names = encoder.get_feature_names_out(categorical_columns)\n",
        "    encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=data.index)\n",
        "\n",
        "    # Process non-categorical columns with MinMaxScaler\n",
        "    non_categorical_columns = [col for col in data.columns if col not in categorical_columns]\n",
        "    scaled_non_categorical = scaler.transform(data[non_categorical_columns])\n",
        "    scaled_non_categorical_df = pd.DataFrame(scaled_non_categorical, columns=non_categorical_columns, index=data.index)\n",
        "\n",
        "    # Combine scaled non-categorical columns and encoded categorical columns\n",
        "    final_data = pd.concat([scaled_non_categorical_df, encoded_df], axis=1)\n",
        "\n",
        "    # Add the first and last columns back to the final data\n",
        "    final_data = pd.concat([first_column, final_data, last_column], axis=1)\n",
        "\n",
        "    # Save the final data to a CSV file\n",
        "    final_data.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"Preprocessed data saved to {output_file}\")\n",
        "\n",
        "\n",
        "# Define file paths and categorical columns\n",
        "normal_file = '../Normal.csv'\n",
        "attack_file = '../Attack.csv'\n",
        "normal_output = 'preprocessed_normal_data.csv'\n",
        "attack_output = 'preprocessed_attack_data.csv'\n",
        "encoder_path = \"onehot_encoder.pkl\"\n",
        "scaler_path = \"minmax_scaler.pkl\"\n",
        "categorical_columns = ['MV101', 'P101', 'P102', 'MV201', 'P201',\n",
        "                       'P202', 'P203', 'P204', 'P205', 'P206', 'MV301',\n",
        "                       'MV302', 'MV303', 'MV304', 'P301', 'P302', \n",
        "                       'P401', 'P402', 'P403', 'P404', 'UV401', 'P501',\n",
        "                       'P502', 'P601', 'P602', 'P603']\n",
        "\n",
        "# Preprocess Normal.csv and train encoder and scaler\n",
        "df_normal = pd.read_csv(normal_file, encoding='utf-8')\n",
        "data = df_normal.iloc[:, 1:-1].copy()\n",
        "\n",
        "# Train OneHotEncoder on categorical columns\n",
        "categories = [list(range(3))] * len(categorical_columns) # here we build the colums _0, _1, e _2\n",
        "encoder = OneHotEncoder(sparse_output=False, drop=None, categories=categories)\n",
        "encoder.fit(data[categorical_columns])\n",
        "\n",
        "# Save the OneHotEncoder\n",
        "joblib.dump(encoder, encoder_path)\n",
        "\n",
        "# Train MinMaxScaler on non-categorical columns\n",
        "non_categorical_columns = [col for col in data.columns if col not in categorical_columns]\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data[non_categorical_columns])\n",
        "\n",
        "# Save the MinMaxScaler\n",
        "joblib.dump(scaler, scaler_path)\n",
        "\n",
        "# Preprocess Normal.csv\n",
        "preprocess_and_save(normal_file, normal_output, encoder, scaler, categorical_columns)\n",
        "\n",
        "# Preprocess Attack.csv using trained encoder and scaler\n",
        "# preprocess_and_save(attack_file, attack_output, encoder, scaler, categorical_columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CUSUM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsTz8VPi-wDv",
        "outputId": "c37ced18-0114-4a71-bfa3-1904d5eca298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters saved to reference_params.csv, using scaler from minmax_scaler.pkl, and encoder from onehot_encoder.pkl\n"
          ]
        }
      ],
      "source": [
        "# @title CUSUM\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def save_cusum_parameters(df, output_file, scaler_file=\"minmax_scaler.pkl\", encoder_file=\"onehot_encoder.pkl\", threshold_scale=3):\n",
        "    \"\"\"\n",
        "    Reads a CSV file, applies OneHotEncoding and MinMaxScaling using pre-trained encoder and scaler,\n",
        "    and saves reference values and thresholds for each feature to a file.\n",
        "\n",
        "    Parameters:\n",
        "        df\n",
        "        output_file (str): Path to save the reference parameters as a CSV.\n",
        "        scaler_file (str): Path to load the MinMaxScaler.\n",
        "        encoder_file (str): Path to load the OneHotEncoder.\n",
        "        threshold_scale (float): Scaling factor for the decision threshold based on standard deviation.\n",
        "    \"\"\"\n",
        "    # Load the pre-trained encoder and scaler\n",
        "    encoder = joblib.load(encoder_file)\n",
        "    scaler = joblib.load(scaler_file)\n",
        "\n",
        "    # Identify categorical and numerical columns\n",
        "    categorical_columns = ['P601', 'P602', 'P603']  # Update with actual categorical columns\n",
        "    numerical_columns = [col for col in df.columns if col not in categorical_columns]\n",
        "\n",
        "    # Apply One-Hot Encoding using the pre-trained encoder\n",
        "    encoded_features = encoder.transform(df[categorical_columns])\n",
        "    encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_columns))\n",
        "\n",
        "    # Apply MinMax Scaling using the pre-trained scaler\n",
        "    scaled_features = scaler.transform(df[numerical_columns])\n",
        "    scaled_df = pd.DataFrame(scaled_features, columns=numerical_columns)\n",
        "\n",
        "    # Combine encoded and scaled data\n",
        "    processed_df = pd.concat([encoded_df, scaled_df], axis=1)\n",
        "\n",
        "    # Calculate parameters for each feature\n",
        "    parameters = {\n",
        "        \"Feature\": [],\n",
        "        \"ReferenceValue\": [],\n",
        "        \"DriftThreshold\": [],\n",
        "        \"DecisionThreshold\": [],\n",
        "    }\n",
        "\n",
        "    for column_name in processed_df.columns:\n",
        "        values = processed_df[column_name]\n",
        "        reference_value = values.mean()\n",
        "        drift_threshold = values.std() * 0.1\n",
        "        decision_threshold = values.std() * threshold_scale\n",
        "\n",
        "        parameters[\"Feature\"].append(column_name)\n",
        "        parameters[\"ReferenceValue\"].append(reference_value)\n",
        "        parameters[\"DriftThreshold\"].append(drift_threshold)\n",
        "        parameters[\"DecisionThreshold\"].append(decision_threshold)\n",
        "\n",
        "    # Save to CSV\n",
        "    pd.DataFrame(parameters).to_csv(output_file, index=False)\n",
        "    print(f\"Parameters saved to {output_file}, using scaler from {scaler_file}, and encoder from {encoder_file}\")\n",
        "\n",
        "\n",
        "output_csv = \"reference_params.csv\"\n",
        "scaler_file = \"minmax_scaler.pkl\"\n",
        "encoder_file = \"onehot_encoder.pkl\"\n",
        "save_cusum_parameters(df, output_csv, scaler_file, encoder_file)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dslab1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

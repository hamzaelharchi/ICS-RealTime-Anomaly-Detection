{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters saved to reference_params.csv\n",
      "Normal/Attack\n",
      "Normal    7219\n",
      "Attack    2781\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10000, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 138\u001b[0m\n\u001b[0;32m    135\u001b[0m results \u001b[38;5;241m=\u001b[39m test_cusum_results(attack_data, CUSUM_PARAMETERS)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Evaluate results\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_cusum_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics)\n",
      "Cell \u001b[1;32mIn[28], line 106\u001b[0m, in \u001b[0;36mevaluate_cusum_results\u001b[1;34m(results, attack_labels)\u001b[0m\n\u001b[0;32m    103\u001b[0m overall_drift_detected \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mgroupby(results\u001b[38;5;241m.\u001b[39mindex)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDriftDetected\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattack_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverall_drift_detected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(attack_labels, overall_drift_detected)\n\u001b[0;32m    108\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(attack_labels, overall_drift_detected)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\dslab1\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\dslab1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\dslab1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\dslab1\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10000, 0]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def save_cusum_parameters_from_preprocessed_data(preprocessed_data, output_file, threshold_scale=3):\n",
    "    \"\"\"\n",
    "    Calculates and saves reference values and thresholds for each feature from preprocessed data.\n",
    "\n",
    "    Parameters:\n",
    "        preprocessed_data (pd.DataFrame): Preprocessed data (encoded and scaled).\n",
    "        output_file (str): Path to save the reference parameters as a CSV.\n",
    "        threshold_scale (float): Scaling factor for the decision threshold based on standard deviation.\n",
    "    \"\"\"\n",
    "\n",
    "  \n",
    "    # Calculate parameters for each featu√°re\n",
    "    parameters = {\n",
    "        \"Feature\": [],\n",
    "        \"ReferenceValue\": [],\n",
    "        \"DriftThreshold\": [],\n",
    "        \"DecisionThreshold\": [],\n",
    "    }\n",
    "\n",
    "    for column_name in preprocessed_data.columns:\n",
    "        values = preprocessed_data[column_name]\n",
    "        reference_value = values.mean()\n",
    "        drift_threshold = values.std() * 0.1\n",
    "        decision_threshold = values.std() * threshold_scale\n",
    "\n",
    "        parameters[\"Feature\"].append(column_name)\n",
    "        parameters[\"ReferenceValue\"].append(reference_value)\n",
    "        parameters[\"DriftThreshold\"].append(drift_threshold)\n",
    "        parameters[\"DecisionThreshold\"].append(decision_threshold)\n",
    "\n",
    "    # Save to CSV\n",
    "    pd.DataFrame(parameters).to_csv(output_file, index=False)\n",
    "    print(f\"Parameters saved to {output_file}\")\n",
    "\n",
    "def load_cusum_parameters(file_path):\n",
    "    \"\"\"\n",
    "    Loads CUSUM parameters from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the CUSUM parameters CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the CUSUM parameters.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def test_cusum_results(attack_data, cusum_parameters):\n",
    "    \"\"\"\n",
    "    Tests CUSUM results on the attack dataset.\n",
    "\n",
    "    Parameters:\n",
    "        attack_data (pd.DataFrame): The attack dataset with preprocessed features.\n",
    "        cusum_parameters (pd.DataFrame): DataFrame containing the CUSUM reference parameters.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Results indicating whether each feature exceeds thresholds.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"Feature\": [],\n",
    "        \"ReferenceValue\": [],\n",
    "        \"DriftThreshold\": [],\n",
    "        \"DecisionThreshold\": [],\n",
    "        \"MaxDeviation\": [],\n",
    "        \"DriftDetected\": [],\n",
    "        \"DecisionThresholdExceeded\": []\n",
    "    }\n",
    "\n",
    "    for _, row in cusum_parameters.iterrows():\n",
    "        feature = row[\"Feature\"]\n",
    "        reference_value = row[\"ReferenceValue\"]\n",
    "        drift_threshold = row[\"DriftThreshold\"]\n",
    "        decision_threshold = row[\"DecisionThreshold\"]\n",
    "\n",
    "        if feature in attack_data.columns:\n",
    "            deviations = attack_data[feature] - reference_value\n",
    "            max_deviation = deviations.abs().max()\n",
    "\n",
    "            results[\"Feature\"].append(feature)\n",
    "            results[\"ReferenceValue\"].append(reference_value)\n",
    "            results[\"DriftThreshold\"].append(drift_threshold)\n",
    "            results[\"DecisionThreshold\"].append(decision_threshold)\n",
    "            results[\"MaxDeviation\"].append(max_deviation)\n",
    "            results[\"DriftDetected\"].append(max_deviation > drift_threshold)\n",
    "            results[\"DecisionThresholdExceeded\"].append(max_deviation > decision_threshold)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def evaluate_cusum_results(results, attack_labels):\n",
    "    \"\"\"\n",
    "    Compares CUSUM results with attack labels and computes metrics.\n",
    "\n",
    "    Parameters:\n",
    "        results (pd.DataFrame): CUSUM results with DriftDetected.\n",
    "        attack_labels (pd.Series): True labels for the attack dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing accuracy, precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "    # Aggregate drift detections across all features\n",
    "    overall_drift_detected = results.groupby(results.index)[\"DriftDetected\"].any()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(attack_labels, overall_drift_detected)\n",
    "    precision = precision_score(attack_labels, overall_drift_detected)\n",
    "    recall = recall_score(attack_labels, overall_drift_detected)\n",
    "    f1 = f1_score(attack_labels, overall_drift_detected)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "preprocessed_normal_data = pd.DataFrame({  # Example DataFrame; replace with your actual preprocessed data\n",
    "    \"feature1\": [0.1, 0.2, 0.3, 0.4],\n",
    "    \"feature2\": [0.5, 0.6, 0.7, 0.8],\n",
    "    \"feature3\": [0.9, 1.0, 1.1, 1.2]\n",
    "})\n",
    "\n",
    "output_csv = \"reference_params.csv\"\n",
    "save_cusum_parameters_from_preprocessed_data(preprocessed_normal_data, output_csv)\n",
    "\n",
    "CUSUM_PARAMETERS = load_cusum_parameters(\"reference_params.csv\")\n",
    "attack_data = pd.read_csv('../Preprocessing/preprocessed_attack_data.csv', encoding='utf-8')\n",
    "\n",
    "# Separate labels from the dataset\n",
    "attack_labels = attack_data.pop(\"Normal/Attack\")\n",
    "print(attack_labels.value_counts())\n",
    "# Test CUSUM results\n",
    "results = test_cusum_results(attack_data, CUSUM_PARAMETERS)\n",
    "\n",
    "# Evaluate results\n",
    "metrics = evaluate_cusum_results(results, attack_labels)\n",
    "print(\"Evaluation Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>ReferenceValue</th>\n",
       "      <th>DriftThreshold</th>\n",
       "      <th>DecisionThreshold</th>\n",
       "      <th>MaxDeviation</th>\n",
       "      <th>DriftDetected</th>\n",
       "      <th>DecisionThresholdExceeded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Feature, ReferenceValue, DriftThreshold, DecisionThreshold, MaxDeviation, DriftDetected, DecisionThresholdExceeded]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dslab1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest
import pickle
import os
def dataEngineering(filepath, actuators_NAMES):
    # Load data
    normal_data = pd.read_csv(filepath)
    normal_data.set_index('Timestamp', inplace=True)

    if 'Normal/Attack' in normal_data.columns:
        normal_attack = normal_data['Normal/Attack']
    else:
        normal_attack = None
        print("Warning: 'Normal/Attack' column not found.")

    normal_data = normal_data.iloc[:, :-1]

    to_drop = ['P402' ,'P203','FIT501','FIT504','P501' ,'FIT502','AIT502' ,'FIT201' ,'MV101','PIT501','PIT503' ,'AIT504' ,'MV201' ,'MV302' ,'FIT503','P302' ,'FIT301' ,'UV401']

    remained_cols= [i for i in normal_data.columns if i not in to_drop]

    normal_data.drop(columns=to_drop, inplace=True)

    actuators_NAMES = [col for col in actuators_NAMES if col in normal_data.columns]

    sensors = normal_data.drop(columns=actuators_NAMES)

    sens_cols = sensors.columns
    print(sens_cols)
    actuators = normal_data[actuators_NAMES]

    scaler = MinMaxScaler()
    scaler.fit(sensors)
    sensors = scaler.transform(sensors)

    sensors = pd.DataFrame(sensors, columns=sens_cols)

    actuators_dummies = actuators.copy()
    for actuator in actuators_NAMES:
        actuators_dummies[actuator] = pd.Categorical(actuators_dummies[actuator], categories=[0, 1, 2])
        actuators_dummies = pd.get_dummies(actuators_dummies, columns=[actuator], dtype=int)

    sensors.index = actuators_dummies.index

    allData = pd.concat([sensors, actuators_dummies], axis=1)

    if normal_attack is not None:
        allData['Normal/Attack'] = normal_attack

    return allData, remained_cols



filepath = '/content/drive/MyDrive/swat/SWat_merged.csv'
actuators_NAMES=['P101', 'P102', 'P201', 'P202', 'P204', 'P205', 'P206', 'MV301',
                           'MV303', 'MV304', 'P301', 'P401', 'P403', 'P404', 'P502', 'P601', 'P602', 'P603']
allData , remained_cols= dataEngineering(filepath,actuators_NAMES)
print(allData)
allData.to_csv('/content/drive/MyDrive/swat/SWaT_Train.csv')


y_sampled = allData['Normal/Attack']

X_sampled = allData.drop(columns=['Normal/Attack','Timestamp'], errors='ignore')  # Drop target column safely

print(f"Number of input samples (rows): {X_sampled.shape[0]}")
print(f"Number of input features (columns): {X_sampled.shape[1]}")
print(f"Number of output samples (rows): {y_sampled.shape[0]}")

X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=42)

iso_model = IsolationForest(n_estimators=100, max_samples='auto', contamination=0.01,
                            max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0)
iso_model.fit(X_train_s)

folder_path = "/content/drive/MyDrive/swat"
file_path = os.path.join(folder_path, "iso_model.pickle")

with open(file_path, 'wb') as f:
    pickle.dump(iso_model, f)


y_pred = iso_model.predict(X_test_s)

output_path = os.path.join(folder_path, "iso_predictions2.csv")
predictions_df=pd.DataFrame(y_pred, columns=['Prediction'], index=X_test_s.index)
print(f"Shape of predictions DataFrame: {predictions_df.shape}")


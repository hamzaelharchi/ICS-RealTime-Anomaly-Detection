{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K28fzbB_NXOr"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "import pandas as pd\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_columns = ['MV101', 'P101', 'P102', 'MV201', 'P201',\n",
        "                       'P202', 'P203', 'P204', 'P205', 'P206', 'MV301',\n",
        "                       'MV302', 'MV303', 'MV304', 'P301', 'P302', \n",
        "                       'P401', 'P402', 'P403', 'P404', 'UV401', 'P501',\n",
        "                       'P502', 'P601', 'P602', 'P603']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed data saved to preprocessed_normal_data.csv\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "def preprocess_and_save(input_file, output_file, encoder, scaler, categorical_columns):\n",
        "    \"\"\"\n",
        "    Preprocess the input CSV file by applying OneHotEncoding to categorical columns,\n",
        "    MinMax scaling to non-categorical columns, and combining them into a final dataset.\n",
        "    \"\"\"\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(input_file, encoding='utf-8', nrows=10000)\n",
        "\n",
        "    # Save the first and last columns separately\n",
        "    first_column = df.iloc[:, 0]\n",
        "    last_column = df.iloc[:, -1]\n",
        "\n",
        "    # Exclude the first and last columns for preprocessing\n",
        "    data = df.iloc[:, 1:-1].copy()\n",
        "\n",
        "    # Process categorical columns with OneHotEncoder\n",
        "    encoded_features = encoder.transform(data[categorical_columns])\n",
        "    encoded_feature_names = encoder.get_feature_names_out(categorical_columns)\n",
        "    encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=data.index)\n",
        "\n",
        "    # Process non-categorical columns with MinMaxScaler\n",
        "    non_categorical_columns = [col for col in data.columns if col not in categorical_columns]\n",
        "    scaled_non_categorical = scaler.transform(data[non_categorical_columns])\n",
        "    scaled_non_categorical_df = pd.DataFrame(scaled_non_categorical, columns=non_categorical_columns, index=data.index)\n",
        "\n",
        "    # Combine scaled non-categorical columns and encoded categorical columns\n",
        "    final_data = pd.concat([scaled_non_categorical_df, encoded_df], axis=1)\n",
        "\n",
        "    # Add the first and last columns back to the final data\n",
        "    final_data = pd.concat([first_column, final_data, last_column], axis=1)\n",
        "\n",
        "    # Save the final data to a CSV file\n",
        "    final_data.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"Preprocessed data saved to {output_file}\")\n",
        "\n",
        "\n",
        "# Define file paths and categorical columns\n",
        "normal_file = '../Normal.csv'\n",
        "attack_file = '../Attack.csv'\n",
        "normal_output = 'preprocessed_normal_data.csv'\n",
        "attack_output = 'preprocessed_attack_data.csv'\n",
        "encoder_path = \"onehot_encoder.pkl\"\n",
        "scaler_path = \"minmax_scaler.pkl\"\n",
        "\n",
        "\n",
        "# Preprocess Normal.csv and train encoder and scaler\n",
        "df_normal = pd.read_csv(normal_file, encoding='utf-8')\n",
        "data = df_normal.iloc[:, 1:-1].copy()\n",
        "\n",
        "# Train OneHotEncoder on categorical columns\n",
        "categories = [list(range(3))] * len(categorical_columns) # here we build the colums _0, _1, e _2\n",
        "encoder = OneHotEncoder(sparse_output=False, drop=None, categories=categories)\n",
        "encoder.fit(data[categorical_columns])\n",
        "\n",
        "# Save the OneHotEncoder\n",
        "joblib.dump(encoder, encoder_path)\n",
        "\n",
        "# Train MinMaxScaler on non-categorical columns\n",
        "non_categorical_columns = [col for col in data.columns if col not in categorical_columns]\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data[non_categorical_columns])\n",
        "\n",
        "# Save the MinMaxScaler\n",
        "joblib.dump(scaler, scaler_path)\n",
        "\n",
        "# Preprocess Normal.csv\n",
        "preprocess_and_save(normal_file, normal_output, encoder, scaler, categorical_columns)\n",
        "\n",
        "# Preprocess Attack.csv using trained encoder and scaler\n",
        "# preprocess_and_save(attack_file, attack_output, encoder, scaler, categorical_columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 Timestamp     PCA_1     PCA_2     PCA_3     PCA_4     PCA_5  \\\n",
            "0   28/12/2015 10:00:00 AM -1.059829 -0.049552 -0.260320 -0.127696  0.001428   \n",
            "1   28/12/2015 10:00:01 AM -1.060511 -0.049077 -0.263962 -0.128010  0.002074   \n",
            "2   28/12/2015 10:00:02 AM -1.061154 -0.047027 -0.272267 -0.126532  0.003439   \n",
            "3   28/12/2015 10:00:03 AM -1.062506 -0.045495 -0.280783 -0.127275  0.004903   \n",
            "4   28/12/2015 10:00:04 AM -1.063629 -0.044246 -0.287349 -0.127848  0.005966   \n",
            "\n",
            "      PCA_6     PCA_7     PCA_8     PCA_9  ...    PCA_42        PCA_43  \\\n",
            "0  0.007354  0.002574 -0.143380 -0.013842  ... -0.000017 -2.886347e-07   \n",
            "1  0.007394  0.002471 -0.142775 -0.013821  ...  0.000241 -2.840494e-07   \n",
            "2  0.007495  0.002204 -0.141969 -0.014063  ...  0.000193 -1.012426e-07   \n",
            "3  0.007309  0.001933 -0.141023 -0.014065  ... -0.000016  8.246891e-09   \n",
            "4  0.007190  0.001696 -0.139883 -0.014046  ... -0.000384  3.571765e-07   \n",
            "\n",
            "         PCA_44        PCA_45        PCA_46        PCA_47        PCA_48  \\\n",
            "0 -3.146099e-15 -2.246546e-15 -6.755830e-16  2.002104e-16  2.015301e-15   \n",
            "1  3.513122e-15  3.022200e-15  4.651392e-15  1.898201e-15  6.807455e-15   \n",
            "2  3.906584e-15  1.548004e-15  3.419829e-15  4.135215e-15 -6.693637e-15   \n",
            "3 -2.687451e-16 -1.785382e-15  2.416832e-15  9.529235e-16 -1.264962e-15   \n",
            "4 -2.626097e-16  3.355742e-15  4.392023e-16  1.934898e-15 -1.345318e-15   \n",
            "\n",
            "         PCA_49        PCA_50  Normal/Attack  \n",
            "0  1.476792e-15 -2.231943e-15         Normal  \n",
            "1 -3.178421e-15  1.807285e-15         Normal  \n",
            "2 -9.391821e-16 -3.908924e-15         Normal  \n",
            "3  1.753861e-15 -1.317766e-16         Normal  \n",
            "4  4.166925e-15  6.201211e-15         Normal  \n",
            "\n",
            "[5 rows x 52 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "attack = pd.read_csv('preprocessed_attack_data.csv', encoding='utf-8')\n",
        "\n",
        "# Assuming the first and last columns are to be kept, we exclude them for PCA\n",
        "# If you have a timestamp or other non-numeric column, drop it and apply PCA only to the numeric columns\n",
        "# `iloc[:, 1:-1]` excludes the first and last columns\n",
        "data_features = attack.iloc[:, 1:-1]\n",
        "\n",
        "# Step 3: Apply PCA (e.g., reduce to 50 components)\n",
        "pca = PCA(n_components=50)\n",
        "pca_result = pca.fit_transform(data_features)\n",
        "\n",
        "# Step 4: Create a DataFrame for the PCA result with proper column names\n",
        "pca_columns = [f'PCA_{i+1}' for i in range(pca_result.shape[1])]\n",
        "pca_df = pd.DataFrame(pca_result, columns=pca_columns)\n",
        "\n",
        "# Step 5: Reattach the first and last columns (keeping them unchanged)\n",
        "final_df = pd.concat([attack.iloc[:, [0]], pca_df, attack.iloc[:, [-1]]], axis=1)\n",
        "\n",
        "# Step 6: Save the PCA model for future use\n",
        "joblib.dump(pca, 'pca_model_attack.pkl')\n",
        "joblib.dump(scaler, 'scaler_model.pkl')\n",
        "\n",
        "# Optionally save the resulting DataFrame to CSV (if you need to inspect the output)\n",
        "final_df.to_csv('attack_data_with_pca.csv', index=False)\n",
        "\n",
        "# Display the resulting DataFrame (optional)\n",
        "print(final_df.head())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dslab1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
